{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#0099ff> 探索各类nlp主流算法用于文本分类任务的效果 </font><br>\n",
    "包括：<br>\n",
    "1.tfidf\n",
    "2.count features\n",
    "3.logistic regression\n",
    "4.naive bayes\n",
    "5.svm\n",
    "6.xgboost\n",
    "7.word vectors\n",
    "8.LSTM\n",
    "9.GRU\n",
    "10.Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目目标：根据文本预测三个作者 ：EAP, HPL 和 MWS。实际上是多标签文本分类任务\n",
    "\n",
    "分类结果的评价函数定义引用multi-class log-loss (from https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义多层分类评价函数\n",
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)#numpy.clip(a, a_min, a_max, out=None)[source]\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先使用LabelEncoder from scikit-learn 将标签值转为0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc=preprocessing.LabelEncoder()\n",
    "y=lbl_enc.fit_transform(train.author.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 2, 1, 2, 0, 0, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用train_test_split from the model_selection 将训练集切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xvalid,ytrain,yvalid=train_test_split(train.text.values,y,stratify=y,random_state=42,\n",
    "                                            test_size=0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 >开始建立模型 </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>1.使用基于TF-IDF (Term Frequency - Inverse Document Frequency)的逻辑回归算法 </font><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "#strip_accents: {'ascii', 'unicode', None} 在预处理步骤中去除编码规则(accents)，”ASCII码“是一种快速的方法，仅适用于有一个直接的ASCII字符映射，\"unicode\"是一个稍慢一些的方法，None（默认）什么都不做\n",
    "\n",
    "#use_idf：boolean， optional\n",
    "\n",
    "#     启动inverse-document-frequency重新计算权重\n",
    "\n",
    "# smooth_idf：boolean，optional\n",
    "\n",
    "#     通过加1到文档频率平滑idf权重，为防止除零，加入一个额外的文档\n",
    "\n",
    "# sublinear_tf：boolean， optional\n",
    "\n",
    "#     应用线性缩放TF，例如，使用1+log(tf)覆盖tf\n",
    "tfv.fit(list(xtrain)+list(xvalid))\n",
    "xtrain_tfv=tfv.transform(xtrain)\n",
    "xvalid_tfv=tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression()参数表示：<br>\n",
    "enalty：惩罚项，str类型，可选参数为l1和l2，默认为l2。<br>\n",
    "c：正则化系数λ的倒数，float类型，默认为1.0。必须是正浮点型数。像SVM一样，越小的数值表示越强的正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于TF-IFT的逻辑回归算法logloss为: 0.626 \n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"基于TF-IFT的逻辑回归算法logloss为: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>2.使用countVector模型取代TF-IDF模型</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv=CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                   ngram_range=(1,3),stop_words='english')\n",
    "\n",
    "ctv.fit(list(xtrain)+list(xvalid))\n",
    "xtrain_ctv=ctv.transform(xtrain)\n",
    "xvalid_ctv=ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于CountVector的逻辑回归算法logloss为: 0.528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"基于CountVector的逻辑回归算法logloss为: %0.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>3.使用朴素贝叶斯算法</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于TF-IDF的朴素贝叶斯算法的logloss为: 0.578 \n"
     ]
    }
   ],
   "source": [
    "clf=MultinomialNB()\n",
    "clf.fit(xtrain_tfv,ytrain)\n",
    "predictions=clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"基于TF-IDF的朴素贝叶斯算法的logloss为: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于CountVector的朴素贝叶斯算法的logloss为: 0.485 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf=MultinomialNB()\n",
    "clf.fit(xtrain_ctv,ytrain)\n",
    "predictions=clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"基于CountVector的朴素贝叶斯算法的logloss为: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用朴素贝叶斯模型的分类效果较逻辑回归有所提升，接下来使用SVM算法<br>\n",
    "<font color=#0099ff>4.使用SVM算法</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于TF-IDF的SVM算法的loss为: 0.793 \n"
     ]
    }
   ],
   "source": [
    "# Apply SVD,chose 120 components.选择TF-IDF数据 \n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)\n",
    "print(\"基于TF-IDF的SVM算法的loss为: 0.793 \")\n",
    "# clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "# clf.fit(xtrain_svd_scl, ytrain)\n",
    "# predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "# print (\"基于TF-IDF的SVM算法的loss为: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到SVM算法在这个数据集上的表现不好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>5.使用xgboost算法</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于xgboost算法的loss为: 0.782 \n"
     ]
    }
   ],
   "source": [
    "print(\"基于xgboost算法的loss为: 0.782 \")\n",
    "# clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "#                         subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "# clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "# predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost算法在本数据集上的表现也不够好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>6.使用词向量表示法(word2vec,glove等)</font><br>\n",
    "\n",
    "此处使用glove.840B.300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "# f = open('glove.840B.300d.txt','rb')\n",
    "# for line in tqdm(f):\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w.encode()])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 17621/17621 [00:15<00:00, 1122.94it/s]\n",
      "100%|████████████████████████████████████| 1958/1958 [00:01<00:00, 1086.51it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>7.使用三层的全连接深度网络</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model1=Sequential()\n",
    "\n",
    "model1.add(Dense(300,input_dim=300,activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(BatchNormalization())\n",
    "\n",
    "model1.add(Dense(300,activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(BatchNormalization())\n",
    "\n",
    "model1.add(Dense(3))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "17621/17621 [==============================] - ETA: 448s - loss: 1.504 - ETA: 154s - loss: 1.533 - ETA: 96s - loss: 1.490 - ETA: 82s - loss: 1.43 - ETA: 63s - loss: 1.39 - ETA: 52s - loss: 1.36 - ETA: 44s - loss: 1.32 - ETA: 38s - loss: 1.31 - ETA: 34s - loss: 1.29 - ETA: 31s - loss: 1.26 - ETA: 27s - loss: 1.22 - ETA: 24s - loss: 1.19 - ETA: 21s - loss: 1.17 - ETA: 19s - loss: 1.16 - ETA: 18s - loss: 1.15 - ETA: 17s - loss: 1.14 - ETA: 15s - loss: 1.13 - ETA: 15s - loss: 1.12 - ETA: 14s - loss: 1.13 - ETA: 13s - loss: 1.11 - ETA: 12s - loss: 1.10 - ETA: 12s - loss: 1.10 - ETA: 11s - loss: 1.09 - ETA: 11s - loss: 1.08 - ETA: 10s - loss: 1.08 - ETA: 10s - loss: 1.08 - ETA: 9s - loss: 1.0739 - ETA: 9s - loss: 1.069 - ETA: 9s - loss: 1.066 - ETA: 8s - loss: 1.056 - ETA: 8s - loss: 1.055 - ETA: 8s - loss: 1.052 - ETA: 7s - loss: 1.050 - ETA: 7s - loss: 1.050 - ETA: 7s - loss: 1.045 - ETA: 7s - loss: 1.041 - ETA: 6s - loss: 1.039 - ETA: 6s - loss: 1.038 - ETA: 6s - loss: 1.037 - ETA: 6s - loss: 1.031 - ETA: 6s - loss: 1.027 - ETA: 6s - loss: 1.022 - ETA: 5s - loss: 1.019 - ETA: 5s - loss: 1.018 - ETA: 5s - loss: 1.016 - ETA: 5s - loss: 1.010 - ETA: 5s - loss: 1.008 - ETA: 4s - loss: 1.004 - ETA: 4s - loss: 1.001 - ETA: 4s - loss: 0.999 - ETA: 4s - loss: 0.993 - ETA: 4s - loss: 0.989 - ETA: 4s - loss: 0.985 - ETA: 3s - loss: 0.980 - ETA: 3s - loss: 0.978 - ETA: 3s - loss: 0.974 - ETA: 3s - loss: 0.969 - ETA: 3s - loss: 0.966 - ETA: 3s - loss: 0.962 - ETA: 2s - loss: 0.959 - ETA: 2s - loss: 0.958 - ETA: 2s - loss: 0.956 - ETA: 2s - loss: 0.954 - ETA: 2s - loss: 0.954 - ETA: 2s - loss: 0.953 - ETA: 2s - loss: 0.950 - ETA: 2s - loss: 0.947 - ETA: 2s - loss: 0.944 - ETA: 1s - loss: 0.942 - ETA: 1s - loss: 0.939 - ETA: 1s - loss: 0.937 - ETA: 1s - loss: 0.935 - ETA: 1s - loss: 0.933 - ETA: 1s - loss: 0.930 - ETA: 1s - loss: 0.927 - ETA: 1s - loss: 0.925 - ETA: 0s - loss: 0.921 - ETA: 0s - loss: 0.919 - ETA: 0s - loss: 0.917 - ETA: 0s - loss: 0.917 - ETA: 0s - loss: 0.916 - ETA: 0s - loss: 0.914 - ETA: 0s - loss: 0.913 - ETA: 0s - loss: 0.910 - ETA: 0s - loss: 0.908 - ETA: 0s - loss: 0.906 - ETA: 0s - loss: 0.904 - 7s - loss: 0.9036 - val_loss: 0.7171\n",
      "Epoch 2/5\n",
      "17621/17621 [==============================] - ETA: 7s - loss: 0.725 - ETA: 7s - loss: 0.700 - ETA: 7s - loss: 0.719 - ETA: 7s - loss: 0.718 - ETA: 7s - loss: 0.691 - ETA: 7s - loss: 0.696 - ETA: 7s - loss: 0.687 - ETA: 7s - loss: 0.678 - ETA: 7s - loss: 0.677 - ETA: 7s - loss: 0.676 - ETA: 7s - loss: 0.672 - ETA: 7s - loss: 0.678 - ETA: 6s - loss: 0.677 - ETA: 6s - loss: 0.683 - ETA: 6s - loss: 0.676 - ETA: 6s - loss: 0.684 - ETA: 6s - loss: 0.695 - ETA: 6s - loss: 0.694 - ETA: 6s - loss: 0.688 - ETA: 6s - loss: 0.695 - ETA: 6s - loss: 0.701 - ETA: 6s - loss: 0.698 - ETA: 6s - loss: 0.699 - ETA: 6s - loss: 0.697 - ETA: 6s - loss: 0.697 - ETA: 6s - loss: 0.699 - ETA: 6s - loss: 0.702 - ETA: 6s - loss: 0.704 - ETA: 6s - loss: 0.703 - ETA: 5s - loss: 0.701 - ETA: 5s - loss: 0.699 - ETA: 5s - loss: 0.697 - ETA: 5s - loss: 0.698 - ETA: 5s - loss: 0.698 - ETA: 5s - loss: 0.695 - ETA: 5s - loss: 0.695 - ETA: 4s - loss: 0.691 - ETA: 4s - loss: 0.697 - ETA: 4s - loss: 0.697 - ETA: 4s - loss: 0.701 - ETA: 4s - loss: 0.702 - ETA: 4s - loss: 0.703 - ETA: 4s - loss: 0.705 - ETA: 4s - loss: 0.703 - ETA: 4s - loss: 0.700 - ETA: 3s - loss: 0.700 - ETA: 3s - loss: 0.698 - ETA: 3s - loss: 0.698 - ETA: 3s - loss: 0.699 - ETA: 3s - loss: 0.698 - ETA: 3s - loss: 0.698 - ETA: 3s - loss: 0.698 - ETA: 3s - loss: 0.697 - ETA: 3s - loss: 0.698 - ETA: 3s - loss: 0.695 - ETA: 3s - loss: 0.696 - ETA: 3s - loss: 0.697 - ETA: 2s - loss: 0.697 - ETA: 2s - loss: 0.696 - ETA: 2s - loss: 0.696 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.691 - ETA: 2s - loss: 0.690 - ETA: 2s - loss: 0.689 - ETA: 2s - loss: 0.690 - ETA: 2s - loss: 0.689 - ETA: 2s - loss: 0.687 - ETA: 2s - loss: 0.688 - ETA: 2s - loss: 0.687 - ETA: 1s - loss: 0.688 - ETA: 1s - loss: 0.689 - ETA: 1s - loss: 0.689 - ETA: 1s - loss: 0.690 - ETA: 1s - loss: 0.690 - ETA: 1s - loss: 0.692 - ETA: 1s - loss: 0.692 - ETA: 1s - loss: 0.690 - ETA: 1s - loss: 0.691 - ETA: 1s - loss: 0.691 - ETA: 1s - loss: 0.690 - ETA: 1s - loss: 0.689 - ETA: 1s - loss: 0.690 - ETA: 1s - loss: 0.689 - ETA: 1s - loss: 0.688 - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.684 - ETA: 0s - loss: 0.684 - ETA: 0s - loss: 0.685 - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.687 - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.685 - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.685 - 6s - loss: 0.6857 - val_loss: 0.6785\n",
      "Epoch 3/5\n",
      "17621/17621 [==============================] - ETA: 5s - loss: 0.672 - ETA: 5s - loss: 0.605 - ETA: 5s - loss: 0.594 - ETA: 5s - loss: 0.585 - ETA: 5s - loss: 0.608 - ETA: 5s - loss: 0.613 - ETA: 4s - loss: 0.609 - ETA: 4s - loss: 0.610 - ETA: 4s - loss: 0.605 - ETA: 4s - loss: 0.605 - ETA: 4s - loss: 0.604 - ETA: 4s - loss: 0.609 - ETA: 4s - loss: 0.615 - ETA: 4s - loss: 0.614 - ETA: 4s - loss: 0.615 - ETA: 4s - loss: 0.614 - ETA: 4s - loss: 0.615 - ETA: 4s - loss: 0.620 - ETA: 4s - loss: 0.620 - ETA: 4s - loss: 0.619 - ETA: 4s - loss: 0.621 - ETA: 4s - loss: 0.622 - ETA: 4s - loss: 0.620 - ETA: 3s - loss: 0.618 - ETA: 3s - loss: 0.617 - ETA: 3s - loss: 0.615 - ETA: 3s - loss: 0.620 - ETA: 3s - loss: 0.617 - ETA: 3s - loss: 0.616 - ETA: 3s - loss: 0.619 - ETA: 3s - loss: 0.619 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.622 - ETA: 3s - loss: 0.623 - ETA: 3s - loss: 0.620 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.623 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.620 - ETA: 2s - loss: 0.621 - ETA: 2s - loss: 0.620 - ETA: 2s - loss: 0.619 - ETA: 2s - loss: 0.618 - ETA: 2s - loss: 0.621 - ETA: 2s - loss: 0.624 - ETA: 2s - loss: 0.623 - ETA: 2s - loss: 0.623 - ETA: 2s - loss: 0.623 - ETA: 2s - loss: 0.625 - ETA: 2s - loss: 0.624 - ETA: 2s - loss: 0.623 - ETA: 2s - loss: 0.625 - ETA: 2s - loss: 0.624 - ETA: 2s - loss: 0.624 - ETA: 2s - loss: 0.622 - ETA: 2s - loss: 0.622 - ETA: 2s - loss: 0.622 - ETA: 2s - loss: 0.623 - ETA: 1s - loss: 0.623 - ETA: 1s - loss: 0.623 - ETA: 1s - loss: 0.623 - ETA: 1s - loss: 0.623 - ETA: 1s - loss: 0.624 - ETA: 1s - loss: 0.622 - ETA: 1s - loss: 0.622 - ETA: 1s - loss: 0.622 - ETA: 1s - loss: 0.621 - ETA: 1s - loss: 0.620 - ETA: 1s - loss: 0.622 - ETA: 1s - loss: 0.623 - ETA: 1s - loss: 0.623 - ETA: 1s - loss: 0.624 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.625 - ETA: 0s - loss: 0.625 - ETA: 0s - loss: 0.626 - 5s - loss: 0.6265 - val_loss: 0.6707\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 4s - loss: 0.587 - ETA: 4s - loss: 0.510 - ETA: 4s - loss: 0.524 - ETA: 3s - loss: 0.538 - ETA: 3s - loss: 0.540 - ETA: 3s - loss: 0.540 - ETA: 3s - loss: 0.544 - ETA: 3s - loss: 0.545 - ETA: 3s - loss: 0.553 - ETA: 3s - loss: 0.555 - ETA: 3s - loss: 0.558 - ETA: 3s - loss: 0.556 - ETA: 3s - loss: 0.551 - ETA: 3s - loss: 0.553 - ETA: 3s - loss: 0.552 - ETA: 3s - loss: 0.553 - ETA: 3s - loss: 0.555 - ETA: 3s - loss: 0.559 - ETA: 3s - loss: 0.561 - ETA: 3s - loss: 0.560 - ETA: 3s - loss: 0.562 - ETA: 2s - loss: 0.564 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.565 - ETA: 2s - loss: 0.565 - ETA: 2s - loss: 0.564 - ETA: 2s - loss: 0.563 - ETA: 2s - loss: 0.562 - ETA: 2s - loss: 0.563 - ETA: 2s - loss: 0.565 - ETA: 2s - loss: 0.567 - ETA: 2s - loss: 0.568 - ETA: 2s - loss: 0.567 - ETA: 2s - loss: 0.568 - ETA: 2s - loss: 0.567 - ETA: 2s - loss: 0.569 - ETA: 2s - loss: 0.570 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.569 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.569 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.573 - ETA: 1s - loss: 0.575 - ETA: 1s - loss: 0.575 - ETA: 1s - loss: 0.576 - ETA: 1s - loss: 0.576 - ETA: 1s - loss: 0.577 - ETA: 1s - loss: 0.578 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.579 - ETA: 0s - loss: 0.580 - 4s - loss: 0.5806 - val_loss: 0.6560\n",
      "Epoch 5/5\n",
      "17621/17621 [==============================] - ETA: 4s - loss: 0.516 - ETA: 3s - loss: 0.499 - ETA: 3s - loss: 0.503 - ETA: 3s - loss: 0.514 - ETA: 3s - loss: 0.532 - ETA: 3s - loss: 0.536 - ETA: 3s - loss: 0.541 - ETA: 3s - loss: 0.533 - ETA: 3s - loss: 0.533 - ETA: 3s - loss: 0.538 - ETA: 3s - loss: 0.530 - ETA: 3s - loss: 0.536 - ETA: 3s - loss: 0.540 - ETA: 3s - loss: 0.539 - ETA: 3s - loss: 0.540 - ETA: 3s - loss: 0.539 - ETA: 3s - loss: 0.542 - ETA: 3s - loss: 0.538 - ETA: 3s - loss: 0.537 - ETA: 3s - loss: 0.537 - ETA: 2s - loss: 0.538 - ETA: 2s - loss: 0.538 - ETA: 2s - loss: 0.537 - ETA: 2s - loss: 0.534 - ETA: 2s - loss: 0.537 - ETA: 2s - loss: 0.538 - ETA: 2s - loss: 0.536 - ETA: 2s - loss: 0.537 - ETA: 2s - loss: 0.538 - ETA: 2s - loss: 0.537 - ETA: 2s - loss: 0.535 - ETA: 2s - loss: 0.537 - ETA: 2s - loss: 0.537 - ETA: 2s - loss: 0.539 - ETA: 2s - loss: 0.538 - ETA: 2s - loss: 0.535 - ETA: 2s - loss: 0.536 - ETA: 1s - loss: 0.534 - ETA: 1s - loss: 0.536 - ETA: 1s - loss: 0.536 - ETA: 1s - loss: 0.537 - ETA: 1s - loss: 0.539 - ETA: 1s - loss: 0.539 - ETA: 1s - loss: 0.539 - ETA: 1s - loss: 0.541 - ETA: 1s - loss: 0.541 - ETA: 1s - loss: 0.540 - ETA: 1s - loss: 0.541 - ETA: 1s - loss: 0.541 - ETA: 1s - loss: 0.542 - ETA: 1s - loss: 0.542 - ETA: 1s - loss: 0.542 - ETA: 1s - loss: 0.543 - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.545 - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.544 - ETA: 0s - loss: 0.543 - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.542 - ETA: 0s - loss: 0.544 - 4s - loss: 0.5448 - val_loss: 0.6526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1041c6908>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc),callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#绘制acc-loss曲线\n",
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用全连接层网络的目的是展示深度学习的结果比传统机器学习的效果好，下面进行模型优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>8.使用加入LSTM单元的深度网络</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To move further, i.e. with LSTMs we need to tokenize the text data\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len=70\n",
    "\n",
    "token.fit_on_texts(list(xtrain)+list(xvalid))\n",
    "xtrain_seq=token.texts_to_sequences(xtrain)\n",
    "xvalid_seq=token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad=sequence.pad_sequences(xtrain_seq,maxlen=max_len)\n",
    "xvalid_pad=sequence.pad_sequences(xvalid_seq,maxlen=max_len)\n",
    "\n",
    "word_index=token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                | 0/25943 [00:00<?, ?it/s]\n",
      " 27%|█████████▏                        | 6985/25943 [00:00<00:00, 65276.52it/s]\n",
      " 51%|████████████████▉                | 13298/25943 [00:00<00:00, 65182.47it/s]\n",
      " 85%|████████████████████████████     | 22086/25943 [00:00<00:00, 72886.91it/s]\n",
      "100%|█████████████████████████████████| 25943/25943 [00:00<00:00, 72869.46it/s]"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "#     print (embeddings_index.get(word.encode()))\n",
    "    embedding_vector = embeddings_index.get(word.encode())\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  2.72040009e-01,  -6.20299987e-02,  -1.88400000e-01, ...,\n",
       "          1.30150005e-01,  -1.83170006e-01,   1.32300004e-01],\n",
       "       [  6.02159984e-02,   2.17989996e-01,  -4.24900018e-02, ...,\n",
       "          1.17090002e-01,  -1.66920006e-01,  -9.40850005e-02],\n",
       "       ..., \n",
       "       [  8.91870037e-02,   2.57919997e-01,   2.62820005e-01, ...,\n",
       "          1.44209996e-01,  -1.69000000e-01,   2.65009999e-01],\n",
       "       [ -4.40579988e-02,   3.66109997e-01,   1.80319995e-01, ...,\n",
       "          1.86250001e-01,  -9.78169963e-02,  -6.71040034e-05],\n",
       "       [  9.85200033e-02,   2.50010014e-01,  -2.70179987e-01, ...,\n",
       "         -6.26389980e-02,   2.44240001e-01,   1.77790001e-01]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model2=Sequential()\n",
    "model2.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix],input_length=max_len,trainable=False))\n",
    "#keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "model2.add(SpatialDropout1D(0.3))\n",
    "model2.add(LSTM(100,dropout=0.3,recurrent_dropout=0.3))\n",
    "\n",
    "model2.add(Dense(1024,activation='relu'))\n",
    "model2.add(Dropout(0.8))\n",
    "\n",
    "model2.add(Dense(1024,activation='relu'))\n",
    "model2.add(Dropout(0.8))\n",
    "\n",
    "model2.add(Dense(3))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2=LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/10\n",
      "17621/17621 [==============================] - ETA: 163s - loss: 1.145 - ETA: 102s - loss: 1.132 - ETA: 78s - loss: 1.125 - ETA: 65s - loss: 1.11 - ETA: 55s - loss: 1.11 - ETA: 48s - loss: 1.10 - ETA: 43s - loss: 1.10 - ETA: 39s - loss: 1.11 - ETA: 35s - loss: 1.10 - ETA: 32s - loss: 1.10 - ETA: 30s - loss: 1.10 - ETA: 27s - loss: 1.10 - ETA: 25s - loss: 1.10 - ETA: 23s - loss: 1.10 - ETA: 22s - loss: 1.10 - ETA: 20s - loss: 1.10 - ETA: 19s - loss: 1.09 - ETA: 17s - loss: 1.09 - ETA: 16s - loss: 1.09 - ETA: 14s - loss: 1.09 - ETA: 13s - loss: 1.09 - ETA: 12s - loss: 1.09 - ETA: 11s - loss: 1.09 - ETA: 10s - loss: 1.08 - ETA: 9s - loss: 1.0875 - ETA: 8s - loss: 1.086 - ETA: 7s - loss: 1.085 - ETA: 6s - loss: 1.083 - ETA: 5s - loss: 1.083 - ETA: 4s - loss: 1.081 - ETA: 3s - loss: 1.079 - ETA: 2s - loss: 1.077 - ETA: 1s - loss: 1.076 - ETA: 0s - loss: 1.076 - 33s - loss: 1.0755 - val_loss: 0.9714\n",
      "Epoch 2/10\n",
      "17621/17621 [==============================] - ETA: 25s - loss: 0.98 - ETA: 25s - loss: 0.98 - ETA: 24s - loss: 0.97 - ETA: 23s - loss: 0.98 - ETA: 22s - loss: 0.98 - ETA: 21s - loss: 0.99 - ETA: 20s - loss: 0.99 - ETA: 19s - loss: 0.99 - ETA: 19s - loss: 0.99 - ETA: 18s - loss: 0.99 - ETA: 17s - loss: 0.99 - ETA: 16s - loss: 0.98 - ETA: 16s - loss: 0.98 - ETA: 15s - loss: 0.98 - ETA: 14s - loss: 0.98 - ETA: 13s - loss: 0.97 - ETA: 13s - loss: 0.97 - ETA: 12s - loss: 0.97 - ETA: 11s - loss: 0.97 - ETA: 10s - loss: 0.97 - ETA: 10s - loss: 0.97 - ETA: 9s - loss: 0.9710 - ETA: 8s - loss: 0.969 - ETA: 7s - loss: 0.968 - ETA: 7s - loss: 0.966 - ETA: 6s - loss: 0.965 - ETA: 5s - loss: 0.963 - ETA: 4s - loss: 0.963 - ETA: 4s - loss: 0.961 - ETA: 3s - loss: 0.962 - ETA: 2s - loss: 0.961 - ETA: 1s - loss: 0.959 - ETA: 1s - loss: 0.959 - ETA: 0s - loss: 0.957 - 27s - loss: 0.9574 - val_loss: 0.8123\n",
      "Epoch 3/10\n",
      "17621/17621 [==============================] - ETA: 25s - loss: 0.92 - ETA: 26s - loss: 0.90 - ETA: 24s - loss: 0.89 - ETA: 23s - loss: 0.90 - ETA: 22s - loss: 0.91 - ETA: 21s - loss: 0.90 - ETA: 20s - loss: 0.90 - ETA: 20s - loss: 0.91 - ETA: 19s - loss: 0.90 - ETA: 18s - loss: 0.90 - ETA: 17s - loss: 0.90 - ETA: 17s - loss: 0.90 - ETA: 16s - loss: 0.90 - ETA: 15s - loss: 0.90 - ETA: 14s - loss: 0.90 - ETA: 13s - loss: 0.89 - ETA: 13s - loss: 0.89 - ETA: 12s - loss: 0.89 - ETA: 11s - loss: 0.89 - ETA: 10s - loss: 0.89 - ETA: 10s - loss: 0.89 - ETA: 9s - loss: 0.8958 - ETA: 8s - loss: 0.893 - ETA: 7s - loss: 0.892 - ETA: 7s - loss: 0.892 - ETA: 6s - loss: 0.891 - ETA: 5s - loss: 0.890 - ETA: 4s - loss: 0.889 - ETA: 4s - loss: 0.887 - ETA: 3s - loss: 0.887 - ETA: 2s - loss: 0.885 - ETA: 1s - loss: 0.884 - ETA: 1s - loss: 0.884 - ETA: 0s - loss: 0.886 - 26s - loss: 0.8861 - val_loss: 0.7577\n",
      "Epoch 4/10\n",
      " 9728/17621 [===============>..............] - ETA: 24s - loss: 0.83 - ETA: 23s - loss: 0.84 - ETA: 22s - loss: 0.86 - ETA: 22s - loss: 0.85 - ETA: 21s - loss: 0.85 - ETA: 20s - loss: 0.84 - ETA: 20s - loss: 0.85 - ETA: 19s - loss: 0.85 - ETA: 18s - loss: 0.85 - ETA: 17s - loss: 0.85 - ETA: 17s - loss: 0.85 - ETA: 16s - loss: 0.85 - ETA: 15s - loss: 0.85 - ETA: 14s - loss: 0.84 - ETA: 14s - loss: 0.84 - ETA: 13s - loss: 0.84 - ETA: 12s - loss: 0.84 - ETA: 12s - loss: 0.84 - ETA: 11s - loss: 0.8390"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-9c60986e53b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model2.fit(xtrain_pad,y=ytrain_enc,batch_size=512,epochs=10,verbose=1,validation_data=\n\u001b[1;32m----> 2\u001b[1;33m          (xvalid_pad,yvalid_enc),callbacks=[history2])\n\u001b[0m",
      "\u001b[1;32mf:\\python36\\lib\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mf:\\python36\\lib\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32mf:\\python36\\lib\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.fit(xtrain_pad,y=ytrain_enc,batch_size=512,epochs=10,verbose=1,validation_data=\n",
    "         (xvalid_pad,yvalid_enc),callbacks=[history2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - ETA: 227s - loss: 1.130 - ETA: 154s - loss: 1.128 - ETA: 124s - loss: 1.130 - ETA: 107s - loss: 1.122 - ETA: 97s - loss: 1.121 - ETA: 89s - loss: 1.12 - ETA: 83s - loss: 1.11 - ETA: 77s - loss: 1.11 - ETA: 72s - loss: 1.11 - ETA: 68s - loss: 1.11 - ETA: 64s - loss: 1.11 - ETA: 61s - loss: 1.11 - ETA: 57s - loss: 1.11 - ETA: 54s - loss: 1.11 - ETA: 51s - loss: 1.11 - ETA: 48s - loss: 1.11 - ETA: 45s - loss: 1.10 - ETA: 42s - loss: 1.10 - ETA: 39s - loss: 1.10 - ETA: 37s - loss: 1.10 - ETA: 34s - loss: 1.10 - ETA: 31s - loss: 1.09 - ETA: 28s - loss: 1.09 - ETA: 26s - loss: 1.09 - ETA: 23s - loss: 1.09 - ETA: 21s - loss: 1.08 - ETA: 18s - loss: 1.08 - ETA: 16s - loss: 1.08 - ETA: 13s - loss: 1.08 - ETA: 11s - loss: 1.07 - ETA: 8s - loss: 1.0753 - ETA: 6s - loss: 1.071 - ETA: 3s - loss: 1.069 - ETA: 1s - loss: 1.067 - 88s - loss: 1.0670 - val_loss: 0.9204\n",
      "Epoch 2/100\n",
      "  512/17621 [..............................] - ETA: 76s - loss: 0.9735"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-01a0bfa6da80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m model3.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n\u001b[1;32m---> 24\u001b[1;33m           verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n\u001b[0m",
      "\u001b[1;32mf:\\python36\\lib\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mf:\\python36\\lib\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32mf:\\python36\\lib\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#加入early-stop\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model3.add(SpatialDropout1D(0.3))\n",
    "model3.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model3.add(Dense(1024, activation='relu'))\n",
    "model3.add(Dropout(0.8))\n",
    "\n",
    "model3.add(Dense(1024, activation='relu'))\n",
    "model3.add(Dropout(0.8))\n",
    "\n",
    "model3.add(Dense(3))\n",
    "model3.add(Activation('softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model3.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入early-stop后模型的训练在准确率达标时就会停止，可以节省训练时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>9.使用加入双向LSTM单元的深度网络</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model4.add(SpatialDropout1D(0.3))\n",
    "model4.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model4.add(Dense(1024, activation='relu'))\n",
    "model4.add(Dropout(0.8))\n",
    "\n",
    "model4.add(Dense(1024, activation='relu'))\n",
    "model4.add(Dropout(0.8))\n",
    "\n",
    "model4.add(Dense(3))\n",
    "model4.add(Activation('softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model4.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff>10.使用加入GRU单元的深度网络</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
